{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i3_B4UEQiTfi"
   },
   "source": [
    "# LAB 2 : Data Preprocessing and cleaning Part 1\n",
    "\n",
    "<font color=\"red\"> **<strong>In this lab, your task is to carefully review all instructions and replace the <code>#FIXME#</code> placeholders with the necessary code to ensure everything functions correctly.</strong>**</font>\n",
    "\n",
    "The following tutorial contains Python examples for data preprocessing. Data preprocessing consists of a broad set of techniques for cleaning, selecting, and transforming data to improve data mining analysis. Read the step-by-step instructions below carefully. To execute the code, click on the corresponding cell and press the SHIFT-ENTER keys simultaneous\n",
    "ly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ojs08TG-iTfr"
   },
   "source": [
    "## 4.1 Data Quality Issues\n",
    "\n",
    "Poor data quality can have an adverse effect on data mining. Among the common data quality issues include noise, outliers, missing values, and duplicate data. This section presents examples of Python code to alleviate some of these data quality problems. We begin with an example dataset from the UCI machine learning repository containing information about breast cancer patients. We will first download the dataset using Pandas read_csv() function and display its first 5 data points.\n",
    "\n",
    "**<font color=\"red\">Code:</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qywrNolZiTfr",
    "outputId": "1b5bc3de-1d46-4a9e-9947-41ae16a7643e"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('breast-cancer-wisconsin.data', header='infer')\n",
    "data.columns = ['Sample code', 'Clump Thickness', 'Uniformity of Cell Size', 'Uniformity of Cell Shape',\n",
    "                'Marginal Adhesion', 'Single Epithelial Cell Size', 'Bare Nuclei', 'Bland Chromatin',\n",
    "                'Normal Nucleoli', 'Mitoses','Class']\n",
    "data = data.drop(['Sample code'],axis=1)\n",
    "print('Number of instances = %d' % (data.shape[0]))\n",
    "print('Number of attributes = %d' % (data.shape[1]))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wnyD6h5eiTfu"
   },
   "source": [
    "### 4.1.1 Missing Values\n",
    "\n",
    "It is not unusual for an object to be missing one or more attribute values. In some cases, the information was not collected; while in other cases, some attributes are inapplicable to the data instances. This section presents examples on the different approaches for handling missing values.\n",
    "\n",
    "According to the description of the data (https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(original), the missing values are encoded as '?' in the original data. Our first task is to convert the missing values to NaNs. We can then count the number of missing values in each column of the data.\n",
    "\n",
    "**<font color=\"red\">Code:</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QG-C7yhtiTfu",
    "outputId": "0053bbc3-fbe7-4c9f-cd09-f00e2d8bdbb5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = #FIXME#\n",
    "\n",
    "print('Number of instances = %d' % (#FIXME#))\n",
    "print('Number of attributes = %d' % (#FIXME#))\n",
    "\n",
    "print('Number of missing values:')\n",
    "for col in data.columns:\n",
    "    print('\\t%s: %d' % (#FIXME#))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NYUAYF0jiTfv"
   },
   "source": [
    "Observe that only the 'Bare Nuclei' column contains missing values. In the following example, the missing values in the 'Bare Nuclei' column are replaced by the median value of that column. The values before and after replacement are shown for a subset of the data points.\n",
    "\n",
    "**<font color=\"red\">Code:</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ej5uIyNIiTfw",
    "outputId": "2230106f-2dd5-48a1-9794-eeab1db68bcb"
   },
   "outputs": [],
   "source": [
    "data2 = data['Bare Nuclei']\n",
    "\n",
    "print('Before replacing missing values:')\n",
    "print(data2[20:25])\n",
    "data2 = data2.fillna(#FIXME#)\n",
    "\n",
    "print('\\nAfter replacing missing values:')\n",
    "print(data2[20:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DM_YqthMiTfx"
   },
   "source": [
    "Instead of replacing the missing values, another common approach is to discard the data points that contain missing values. This can be easily accomplished by applying the dropna() function to the data frame.\n",
    "\n",
    "**<font color=\"red\">Code:</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fec1C6hniTfx"
   },
   "outputs": [],
   "source": [
    "print('Number of rows in original data = %d' % (data.shape[0]))\n",
    "\n",
    "data2 = #FIXME#\n",
    "print('Number of rows after discarding missing values = %d' % (data2.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lUCAAb7HiTfy"
   },
   "source": [
    "### 4.1.2 Outliers\n",
    "\n",
    "Outliers are data instances with characteristics that are considerably different from the rest of the dataset. In the example code below, we will draw a boxplot to identify the columns in the table that contain outliers. Note that the values in all columns (except for 'Bare Nuclei') are originally stored as 'int64' whereas the values in the 'Bare Nuclei' column are stored as string objects (since the column initially contains strings such as '?' for representing missing values). Thus, we must  convert the column into numeric values first before creating the boxplot. Otherwise, the column will not be displayed when drawing the boxplot.\n",
    "\n",
    "**<font color=\"red\">Code:</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gkOB43ueiTfy"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "data2 = data.drop(['Class'],axis=1)\n",
    "data2['Bare Nuclei'] = #FIXME#\n",
    "data2.boxplot(figsize=(20,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lXisrSK6iTfz"
   },
   "source": [
    "The boxplots suggest that only 5 of the columns (Marginal Adhesion, Single Epithetial Cell Size, Bland Cromatin, Normal Nucleoli, and Mitoses) contain abnormally high values. To discard the outliers, we can compute the Z-score for each attribute and remove those instances containing attributes with abnormally high or low Z-score (e.g., if Z > 3 or Z <= -3).\n",
    "\n",
    "**<font color=\"red\">Code:</font>**\n",
    "\n",
    "The following code shows the results of standardizing the columns of the data. Note that missing values (NaN) are not affected by the standardization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "weK-U-VriTfz",
    "outputId": "78e585b9-6de6-45cb-e7af-a5ddd49d631a"
   },
   "outputs": [],
   "source": [
    "Z = (data2-data2.mean())/data2.std()\n",
    "Z[20:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BXqk8AHQiTfz"
   },
   "source": [
    "<font color=\"red\">Code:</font>\n",
    "\n",
    "In the code provided, we perform a data filtering operation to identify and remove exammples from a dataset where the values either exceed Z > 3 or fall below Z <= -3. This process helps us in handling outliers in our dataset more effectively.\n",
    "\n",
    "The concept here is to assess each example's values and consider them as potential outliers if they significantly deviate from the mean value. Specifically, Z > 3 or Z <= -3 are used as criteria to identify examples that are far from the mean, assuming that the attributes in these examples follow a normal distribution.\n",
    "\n",
    "The key idea behind this operation is to improve the quality of our data by excluding extreme values that could adversely affect subsequent analyses or modeling efforts. It's a common practice in data preprocessing to ensure that our dataset accurately reflects the underlying patterns and trends in the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EoJAElqCiTfz"
   },
   "outputs": [],
   "source": [
    "print('Number of rows before discarding outliers = %d' % (Z.shape[0]))\n",
    "\n",
    "Z2 = Z.loc[((Z > -3).sum(axis=1)==9) & ((Z <= 3).sum(axis=1)==9),:]\n",
    "print('Number of rows after discarding missing values = %d' % (Z2.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6vCq0pqXiTf0"
   },
   "source": [
    "### 4.1.3 Duplicate Data\n",
    "\n",
    "Some datasets, especially those obtained by merging multiple data sources, may contain duplicates or near duplicate instances. The term deduplication is often used to refer to the process of dealing with duplicate data issues.\n",
    "\n",
    "**<font color=\"red\">Code:</font>**\n",
    "\n",
    "In the following example, we first check for duplicate instances in the breast cancer dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bsVkLpaCiTf2"
   },
   "outputs": [],
   "source": [
    "dups = data.#FIXME#\n",
    "print('Number of duplicate rows = %d' % (dups.sum()))\n",
    "data.loc[[11,28]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I-eqkhmRiTf3"
   },
   "source": [
    "The duplicated() function will return a Boolean array that indicates whether each row is a duplicate of a previous row in the table. The results suggest there are 236 duplicate rows in the breast cancer dataset. For example, the instance with row index 11 has identical attribute values as the instance with row index 28. Although such duplicate rows may correspond to samples for different individuals, in this hypothetical example, we assume that the duplicates are samples taken from the same individual and illustrate below how to remove the duplicated rows.\n",
    "\n",
    "**<font color=\"red\">Code:</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xG6dRdbriTf3"
   },
   "outputs": [],
   "source": [
    "print('Number of rows before discarding duplicates = %d' % (data.shape[0]))\n",
    "data2 = #FIXME#\n",
    "print('Number of rows after discarding duplicates = %d' % (data2.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w2dp64cQiTf3"
   },
   "source": [
    "## 4.2 Aggregation\n",
    "\n",
    "Data aggregation is a preprocessing task where the values of two or more objects are combined into a single object. The motivation for aggregation includes (1) reducing the size of data to be processed, (2) changing the granularity of analysis (from fine-scale to coarser-scale), and (3) improving the stability of the data.\n",
    "\n",
    "In the example below, we will use the daily precipitation time series data for a weather station located at Detroit Metro Airport. The raw data was obtained from the Climate Data Online website (https://www.ncdc.noaa.gov/cdo-web/). The daily precipitation time series will be compared against its monthly values.\n",
    "\n",
    "**<font color=\"red\">Code:</font>**\n",
    "\n",
    "The code below will load the precipitation time series data and draw a line plot of its daily time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bPG025QBiTf3"
   },
   "outputs": [],
   "source": [
    "daily = pd.read_csv('DTW_prec.csv', header='infer')\n",
    "daily.index = pd.to_datetime(daily['DATE'])\n",
    "daily = daily['PRCP']\n",
    "ax = daily.plot(kind='line',figsize=(15,3))\n",
    "ax.set_title('Daily Precipitation (variance = %.4f)' % (daily.var()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5U0XNfdziTf4"
   },
   "source": [
    "Observe that the daily time series appear to be quite chaotic and varies significantly from one time step to another. The time series can be grouped and aggregated by month to obtain the total monthly precipitation values. The resulting time series appears to vary more smoothly compared to the daily time series.\n",
    "\n",
    "You can use `pd.Grouper(freq='M')` to group the rows.\n",
    "\n",
    "**<font color=\"red\">Code:</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hEnqfUvpiTf4"
   },
   "outputs": [],
   "source": [
    "monthly = daily.groupby(#FIXME#).sum()\n",
    "ax = monthly.plot(kind='line',figsize=(15,3))\n",
    "ax.set_title('Monthly Precipitation (variance = %.4f)' % (monthly.var()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EVi2c51QiTf4"
   },
   "source": [
    "In the example below, the daily precipitation time series are grouped and aggregated by year to obtain the annual precipitation values.\n",
    "\n",
    "You can use `pd.Grouper(freq='Y')` to group the rows.\n",
    "\n",
    "**<font color=\"red\">Code:</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jq_F0IuziTf4"
   },
   "outputs": [],
   "source": [
    "annual = daily.groupby(#FIXME#).sum()\n",
    "ax = annual.plot(kind='line',figsize=(15,3))\n",
    "ax.set_title('Annual Precipitation (variance = %.4f)' % (annual.var()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G2kqChHpiTf4"
   },
   "source": [
    "## 4.3 Sampling\n",
    "\n",
    "Sampling is an approach commonly used to facilitate (1) data reduction for exploratory data analysis and scaling up algorithms to big data applications and (2) quantifying uncertainties due to varying data distributions. There are various methods available for data sampling, such as sampling without replacement, where each selected instance is removed from the dataset, and sampling with replacement, where each selected instance is not removed, thus allowing it to be selected more than once in the sample.\n",
    "\n",
    "In the example below, we will apply sampling with replacement and without replacement to the breast cancer dataset obtained from the UCI machine learning repository.\n",
    "\n",
    "**<font color=\"red\">Code:</font>**\n",
    "\n",
    "We initially display the first five records of the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hi3PqdqviTf4"
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c63a1iQ-iTf5"
   },
   "source": [
    "In the following code, a sample of size 3 is randomly selected (without replacement) from the original data.\n",
    "\n",
    "**<font color=\"red\">Code:</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_cv9YjwKiTf5"
   },
   "outputs": [],
   "source": [
    "sample = data.sample(n=3)\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NTzD0yw0iTf5"
   },
   "source": [
    "In the next example, we randomly select 1% of the data (without replacement) and display the selected samples. The random_state argument of the function specifies the seed value of the random number generator.\n",
    "\n",
    "**<font color=\"red\">Code:</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z5943yeviTf5"
   },
   "outputs": [],
   "source": [
    "sample = #FIXME#\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "egGyOLYyiTf5"
   },
   "source": [
    "Finally, we perform a sampling with replacement to create a sample whose size is equal to 1% of the entire data. You should be able to observe duplicate instances in the sample by increasing the sample size.\n",
    "\n",
    "**<font color=\"red\">Code:</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7lfh40FWiTf5"
   },
   "outputs": [],
   "source": [
    "sample = #FIXME#\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UuAlfnJKiTf6"
   },
   "source": [
    "## 4.4 Discretization\n",
    "\n",
    "Discretization is a data preprocessing step that is often used to transform a continuous-valued attribute to a categorical attribute. The example below illustrates two simple but widely-used unsupervised discretization methods (equal width and equal depth) applied to the 'Clump Thickness' attribute of the breast cancer dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vJAJYB9CiTf6"
   },
   "source": [
    "First, we plot a histogram that shows the distribution of the attribute values. The value_counts() function can also be applied to count the frequency of each attribute value.\n",
    "\n",
    "**<font color=\"red\">Code:</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oIHUG54WiTf6"
   },
   "outputs": [],
   "source": [
    "data['Clump Thickness'].hist(bins=10)\n",
    "data['Clump Thickness'].value_counts(sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N8CPB_0wiTf6"
   },
   "source": [
    "For the equal width method, we can apply the cut() function to discretize the attribute into 4 bins of similar interval widths. The value_counts() function can be used to determine the number of instances in each bin.\n",
    "\n",
    "**<font color=\"red\">Code:</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "99fHqenKiTf6"
   },
   "outputs": [],
   "source": [
    "bins = pd.cut(data['Clump Thickness'],4)\n",
    "bins.value_counts(sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pQVI9lo5iTf6"
   },
   "source": [
    "For the equal frequency method, the qcut() function can be used to partition the values into 4 bins such that each bin has nearly the same number of instances.\n",
    "\n",
    "**<font color=\"red\">Code:</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ExPdktGNiTgI"
   },
   "outputs": [],
   "source": [
    "bins = pd.qcut(data['Clump Thickness'],4)\n",
    "bins.value_counts(sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "na1Ersb2iTgJ"
   },
   "source": [
    "## 4.6 Summary\n",
    "\n",
    "This tutorial presents Python programming examples for data preprocessing, including data cleaning (to handle missing values and remove outliers as well as duplicate data), aggregation, sampling, discretization, and dimensionality reduction using principal component analysis.   \n",
    "\n",
    "**<font color='blue'>References:</font>**\n",
    "\n",
    "1. Lichman, M. (2013). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science.\n",
    "\n",
    "2. Mangasarian, O.L. and Wolberg, W. H. (1990). \"Cancer diagnosis via linear programming\", SIAM News, Volume 23, Number 5, pp 1 & 18.\n",
    "\n",
    "3. Wolberg, W.H. and Mangasarian, O.L. (1990). \"Multisurface method of pattern separation for medical diagnosis applied to breast cytology\", Proceedings of the National Academy of Sciences, U.S.A., Volume 87, pp 9193-9196.\n",
    "\n",
    "4. Climate Data Online [https://www.ncdc.noaa.gov/cdo-web/]."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
